# src/build_uid_map_for_truncated_titles.py

from __future__ import annotations

import os
import re
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple, List, Iterable, Optional

from utils.get_safe_path import get_safe_path
from utils.logger import Logger


# ===== Constants =====
LONG_FILENAME_UTF8_BYTES_THRESHOLD = 70  # ç›®å‰æª”åç´„èƒ½ä½¿ç”¨97Byteï¼Œä¸æ’é™¤æœ‰å®¹é‡æ›´å°ã€æå‰æˆªæ–·çš„æƒ…æ³ï¼Œæ‰€ä»¥å¯èƒ½è¦è¨­å¾—æ¯”97å°ä¸€äº›


# ===== Data Models =====
from dataclasses import dataclass
from typing import Dict, Optional

@dataclass
class TruncationMapEntry:
    """truncation_map çš„ value çµæ§‹ï¼š{ uid, full_sentence }"""
    uid: str
    full_sentence: str

    # æ–¹ä¾¿ I/O çš„å°å·¥å…·ï¼ˆå¯é¸ï¼‰
    @staticmethod
    def from_dict(d: dict) -> "TruncationMapEntry":
        return TruncationMapEntry(uid=d["uid"], full_sentence=d["full_sentence"])

    def to_dict(self) -> dict:
        return {"uid": self.uid, "full_sentence": self.full_sentence}


@dataclass
class Indices:
    """è¦æ ¼ï¼šé«˜å±¤æµç¨‹ Step1 æ‰€éœ€çš„ä¸‰å€‹ç´¢å¼•ï¼ˆçš†ç‚ºå¿…å‚™ï¼‰"""
    full_to_uid: Dict[str, str]
    uid_to_expected_full: Dict[str, str]
    uid_to_key: Dict[str, str]

    # æŸ¥è©¢è¼”åŠ©ï¼ˆè®€æ“ä½œï¼‰
    def uid_for_full(self, full_sentence: str) -> Optional[str]:
        return self.full_to_uid.get(full_sentence)

    def key_for_uid(self, uid: str) -> Optional[str]:
        return self.uid_to_key.get(uid)

    def expected_full_for_uid(self, uid: str) -> Optional[str]:
        return self.uid_to_expected_full.get(uid)

    def has_uid(self, uid: str) -> bool:
        return uid in self.uid_to_expected_full

    def has_full(self, full_sentence: str) -> bool:
        return full_sentence in self.full_to_uid

    # æ–°å¢ map æ¢ç›®æ™‚ï¼Œä¿æŒä¸‰ç´¢å¼•åŒæ­¥ï¼ˆå¯«æ“ä½œï¼‰
    def register(self, key: str, entry: TruncationMapEntry) -> None:
        self.full_to_uid[entry.full_sentence] = entry.uid
        self.uid_to_expected_full[entry.uid] = entry.full_sentence
        self.uid_to_key[entry.uid] = key

    # è®Šæ›´ full_sentenceï¼ˆä¾‹å¦‚ F2 åºè™ŸåŒ–ï¼‰æ™‚ï¼Œå®‰å…¨æ›´æ–°ç´¢å¼•
    def update_full_for_uid(self, uid: str, old_full: str, new_full: str) -> None:
        if self.full_to_uid.get(old_full) == uid:
            self.full_to_uid.pop(old_full, None)
        self.full_to_uid[new_full] = uid
        self.uid_to_expected_full[uid] = new_full
        # uid_to_key ä¸è®Šï¼ˆkey ä»æ˜¯è¢«æˆªæ–·çš„ä¸å®Œæ•´èªå¥ï¼‰


@dataclass
class Stats:
    """è¦æ ¼ï¼šğŸ“ŠLog èˆ‡çµ±è¨ˆ æ‰€åˆ—è¨ˆæ•¸å™¨"""
    new_uid_assigned: int = 0
    supplemental_entries_added: int = 0   # è£œç™»éŒ„ï¼æ–°å¢ map æ¢ç›®æ•¸
    renames_to_uid: int = 0               # æ­£åæ•¸ï¼ˆä¸€èˆ¬/Temp â†’ uidï¼‰
    duplicates_deleted: int = 0           # F1 å®Œå…¨é‡è¤‡åˆªé™¤
    conflicts_serialized: int = 0         # åŒé¦–å¥ä¸åŒå…§å®¹ â†’ å·²åºè™ŸåŒ–è™•ç†
    temps_repaired: int = 0               # temp â†’ æ­£å¼ uid
    orphan_targets_seen: int = 0          # map éºå¤±å¯¦é«”åµæ¸¬
    # å…¶ä»–å¯æ“´å……æ¬„ä½â€¦

    # å¾®å·¥å…·ï¼šç´¯åŠ å™¨ï¼Œé¿å…å„è™•æ‰‹å‹• +1
    def inc_new_uid(self): self.new_uid_assigned += 1
    def inc_added(self): self.supplemental_entries_added += 1
    def inc_renamed(self): self.renames_to_uid += 1
    def inc_deleted_dup(self): self.duplicates_deleted += 1
    def inc_serialized(self): self.conflicts_serialized += 1
    def inc_repaired_temp(self): self.temps_repaired += 1
    def inc_orphan(self): self.orphan_targets_seen += 1



# ===== I/O & Map =====
def load_truncation_map(map_path: str) -> Dict[str, TruncationMapEntry]:
    """è®€å– truncation_map.json â†’ ä»¥ key(str)â†’TruncationMapEntry å›å‚³ã€‚
    è¦æ ¼ï¼šmap ç”±ä»–ç¨‹ä¿é¤Šï¼›è‹¥é‡ä¸ä¸€è‡´ï¼Œæœ¬ç¨‹å¼ä¸è¦†å¯«ã€ä¸æŒªç”¨ï¼Œåƒ…è¨˜éŒ„ auditã€‚
    """
    p = get_safe_path(map_path)
    if not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            raw = json.load(f)
        result: Dict[str, TruncationMapEntry] = {}
        for k, v in raw.items():
            # å®¹éŒ¯ï¼šv å¯èƒ½æ˜¯ dict æˆ–å·²æ˜¯æ­£ç¢ºçµæ§‹
            uid = v["uid"] if isinstance(v, dict) else getattr(v, "uid")
            fs = v["full_sentence"] if isinstance(v, dict) else getattr(v, "full_sentence")
            result[k] = TruncationMapEntry(uid=uid, full_sentence=fs)
        return result
    except Exception:
        # è®€å–å¤±æ•— â†’ å›å‚³ç©º mapï¼ˆå¤–éƒ¨ç¨‹å¼ä¿é¤Š mapï¼›é€™è£¡ä¸å˜—è©¦ä¿®å¾©ï¼‰
        return {}


def save_truncation_map(map_path: str, truncation_map: Dict[str, TruncationMapEntry]) -> None:
    """å®‰å…¨å¯«å› truncation_map.jsonï¼ˆç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨ã€UTF-8ï¼‰ã€‚
    """
    p = get_safe_path(map_path)
    os.makedirs(os.path.dirname(p), exist_ok=True)
    serializable = {k: {"uid": v.uid, "full_sentence": v.full_sentence} for k, v in truncation_map.items()}
    with open(p, "w", encoding="utf-8") as f:
        json.dump(serializable, f, ensure_ascii=False, indent=2)


def build_indices_from_map(truncation_map: Dict[str, TruncationMapEntry]) -> Indices:
    """ç”± map å»ºç«‹ full_to_uid / uid_to_expected_full / uid_to_key ä¸‰ç´¢å¼•ã€‚"""
    full_to_uid: Dict[str, str] = {}
    uid_to_expected_full: Dict[str, str] = {}
    uid_to_key: Dict[str, str] = {}
    for k, entry in truncation_map.items():
        full_to_uid[entry.full_sentence] = entry.uid
        uid_to_expected_full[entry.uid] = entry.full_sentence
        uid_to_key[entry.uid] = k
    return Indices(full_to_uid=full_to_uid, uid_to_expected_full=uid_to_expected_full, uid_to_key=uid_to_key)


# ===== File Scanning =====
def iter_vault_md_files(vault_path: str) -> Iterable[Path]:
    """éæ­·æ•´å€‹ Vaultï¼ˆå…¨åŸŸï¼‰æ‰¾å‡ºæ‰€æœ‰ .md æª”ï¼ˆå« uid / é uid / tempï¼‰ï¼Œyield çµ•å°è·¯å¾‘ Pathã€‚"""
    vp = os.path.abspath(vault_path)
    for root, _, files in os.walk(vp):
        for fn in files:
            if fn.lower().endswith(".md"):
                yield Path(get_safe_path(os.path.join(root, fn)))


def read_lines(path: Path) -> List[str]:
    """è®€å–å–®æª”æ‰€æœ‰è¡Œï¼ˆä¿ç•™æ›è¡Œç¬¦ï¼‰ã€‚"""
    with open(get_safe_path(str(path)), "r", encoding="utf-8", errors="ignore") as f:
        return f.readlines()


# ===== Markdown Cleaning =====
def skip_yaml(lines: List[str]) -> List[str]:
    """è¦æ ¼ 1ï¼šè·³é YAML å€å¡Šï¼ˆé¦–è¡Œ/æ¬¡è¡Œçš†ä»¥å–®ç¨ '---' ç‚ºé‚Šç•Œï¼Œå«é‚Šç•Œï¼‰ã€‚"""
    if not lines:
        return lines
    i = 0
    # åƒ…ç•¶ç¬¬ä¸€è¡Œæ˜¯ '---' æ™‚æ‰è¦–ç‚º YAML é–‹é ­
    if lines[0].strip() == "---":
        for i in range(1, len(lines)):
            if lines[i].strip() == "---":
                return lines[i + 1 :]
        # æ²’æœ‰æ”¶å°¾ï¼Œä¿å®ˆèµ·è¦‹ï¼šè¦–ç‚ºæ²’ YAML
        return lines
    return lines


def first_nonempty_line(lines: List[str]) -> str:
    """è¦æ ¼ 1ï¼šå–ç¬¬ä¸€å€‹éç©ºè¡Œï¼ˆå¯èƒ½æ˜¯ list/blockquote èµ·é ­ï¼‰ã€‚"""
    for ln in lines:
        if ln.strip():
            return ln
    return ""


def clean_markdown_line(line: str) -> str:
    """è¦æ ¼ 1ï¼šæ¸…ç† Markdownï¼ˆåƒ…é™ï¼‰
    - è¡Œé¦–æ¸…å–®ç¬¦è™Ÿï¼š- * +
    - è¡Œé¦–æœ‰åºæ¸…å–®ï¼š<æ•¸å­—> æˆ– <æ•¸å­—>. å¾Œæ¥ç©ºç™½
    - æ¨™é¡Œèµ·é ­ï¼šä¸€å€‹æˆ–å¤šå€‹ # å¾Œæ¥ç©ºç™½
    - å€å¡Šå¼•ç”¨ï¼šä¸€å€‹æˆ–å¤šå€‹ > å¾Œæ¥ç©ºç™½
    - è¡Œå…§æ¨£å¼ï¼š**ç²—é«”**ã€*æ–œé«”*ï¼ˆä¸è·¨è¶Š ** çš„æƒ…å½¢ï¼‰ã€`è¡Œå…§ç¨‹å¼ç¢¼`
    - Wikilink å»æ®¼ï¼š
        [[page|alias]] â†’ alias
        [[uid|@full_sentence]] â†’ å» @ çš„ alias
        [[page]] â†’ page
    - å°¾ç¢¼æ—¥æœŸç­‰ï¼šå»é™¤ã€Œçµå°¾çš„ ç©ºç™½ + ç´”æ•¸å­—ã€å°¾ç¢¼ï¼ˆåƒ…ç´”æ•¸å­—ï¼‰
    - å»å‰å¾Œç©ºç™½
    å†ªç­‰ï¼šä¸ç§»é™¤ '(n)' åºè™ŸåŒ–å°¾ç¶´ã€‚
    """
    s = line.lstrip()
    s = re.sub(r"^[-+*]\s+", "", s)               # ç„¡åºæ¸…å–®
    s = re.sub(r"^\d+\.?\s*", "", s)              # æœ‰åºæ¸…å–®
    s = re.sub(r"^#+\s*", "", s)                  # æ¨™é¡Œ
    s = re.sub(r"^>+\s*", "", s)                  # å€å¡Šå¼•ç”¨
    s = re.sub(r"\*\*(.*?)\*\*", r"\1", s)        # ç²—é«”
    s = re.sub(r"(?<!\*)\*(?!\*)([^*]+)\*(?!\*)", r"\1", s)  # æ–œé«”ï¼ˆä¸è·¨ **ï¼‰
    s = re.sub(r"`([^`]+)`", r"\1", s)            # è¡Œå…§ç¨‹å¼ç¢¼

    # Wikilinkï¼š[[page|alias]] / [[uid|@full_sentence]] / [[page]]
    def _wikilink_repl(m: re.Match) -> str:
        page = m.group(1)
        alias = m.group(2)
        if alias is not None:
            alias = alias.lstrip("@")
            return alias
        return page

    s = re.sub(r"\[\[([^\|\]]+)(?:\|([^\]]+))?\]\]", _wikilink_repl, s)

    # å»é™¤å°¾ç«¯ã€Œç©ºç™½ + ç´”æ•¸å­—ã€å°¾ç¢¼ï¼ˆä¸æœƒç§»é™¤ '(n)'ï¼‰
    s = re.sub(r"\s*\d+$", "", s).strip()
    return s


# ===== Filename vs First Sentence (Semantic Truncation) =====
def remove_trailing_number(text: str) -> str:
    """è¦æ ¼ 2ï¼šæª”åé è™•ç†ï¼šç§»é™¤å°¾ç«¯ã€ç©ºç™½ + ç´”æ•¸å­—ã€ã€‚"""
    return re.sub(r"\s*\d+$", "", text.strip())


def compare_filename_and_line(filename: str, cleaned: str) -> Tuple[bool, str]:
    """è¦æ ¼ 2ï¼šèªæ„æ–·å¥åˆ¤å®šï¼ˆé€å­—æ¯”ã€æœ‰æ•ˆå­—å…ƒé›†åˆã€å™ªè²äº’ç›¸æŠµéŠ·ï¼‰
    å›å‚³ï¼š(æ˜¯å¦ç¬¦åˆèªæ„æ–·å¥, ç†ç”±)
    """
    def _is_valid_char(c: str) -> bool:
        return c.isalnum() or c in " -_()[]"

    fn = remove_trailing_number(filename)
    i, j = 0, 0
    while i < len(fn) and j < len(cleaned):
        c1, c2 = fn[i], cleaned[j]
        if c1 == c2:
            i += 1; j += 1
        elif not _is_valid_char(c1) or not _is_valid_char(c2):
            i += 1; j += 1  # å™ªè²äº’ç›¸æŠµéŠ·
        else:
            return False, f"âŒ å­—å…ƒä¸ç¬¦ï¼š'{c1}' â‰  '{c2}' ä½ç½® {i}"
    if i < len(fn):
        return False, f"âŒ æª”åæœªå®Œå…¨åŒ¹é…ï¼šåƒ…æ¯”å°è‡³ {i}/{len(fn)}"
    remaining = cleaned[j:].strip()
    if remaining == "" or re.fullmatch(r"\d+", remaining):
        return False, "âŒ å‰©é¤˜å…§å®¹åƒ…ç‚ºæ•¸å­—æˆ–ç©ºç™½"
    return True, "âœ”ï¸ é¦–å¥åŒ…å«é¡å¤–èªæ„ï¼Œæ§‹æˆèªæ„æ–·å¥"


def is_truncated(filename_clean: str, full_sentence: str, threshold: int = LONG_FILENAME_UTF8_BYTES_THRESHOLD) -> Tuple[bool, str]:
    """è¦æ ¼ 3ï¼šè¢«æˆªæ–·åˆ¤å®šï¼ˆçµ‚æ­¢ç¬¦ / é•·åº¦é–€æª» + è£œè¿°ï¼‰ã€‚"""
    tail = full_sentence[len(filename_clean):].strip() if len(full_sentence) >= len(filename_clean) else ""
    filename_byte_length = len(filename_clean.encode("utf-8"))
    if tail in {".", "?", "!"}:
        return True, f"âœ”ï¸ è£œè¿°ç‚ºç¬¦è™Ÿï¼š'{tail}' â†’ èªå®šç‚ºæˆªæ–·"
    if filename_byte_length >= threshold and tail:
        return True, f"âœ”ï¸ æª”åé•·ä¸”æœ‰è£œè¿°ï¼ˆ{filename_byte_length} bytesï¼‰â†’ èªå®šç‚ºæˆªæ–·"
    return False, f"âŒ æª”åé•·åº¦ {filename_byte_length} bytesï¼Œè£œè¿°éé—œéµ â†’ éæˆªæ–·"


# ===== Key Generation (for uid_XXX not in map / must include) =====
def synthesize_truncation_key_from_cleaned(cleaned: str, threshold: int = LONG_FILENAME_UTF8_BYTES_THRESHOLD) -> str:
    """è¦æ ¼ 6ï¼šä»¥ cleaned åæ¨ã€æ¨™æº–æˆªæ–· keyã€
    1) çµ‚æ­¢ç¬¦æˆªæ–·ï¼ˆå»æ‰å°¾ç«¯å–®ä¸€ . ? !ï¼‰
    2) ä½å…ƒçµ„é–€æª»æˆªæ–·ï¼ˆâ‰¤ thresholdï¼Œå»å°¾ç©ºç™½/ç´”æ•¸å­—ï¼›éœ€åœ¨ UTF-8 å­—å…ƒé‚Šç•Œï¼‰
    3) å¼·åˆ¶æˆªæ–·ï¼ˆç§»é™¤æœ€å¾Œä¸€å€‹èªè©ï¼›å†ä¸è¡Œå‰‡ç§»é™¤æœ€å¾Œä¸€å­—å…ƒï¼‰
    """
    s = cleaned.strip()
    # 1) çµ‚æ­¢ç¬¦
    if s and s[-1] in ".?!":
        return s[:-1].rstrip()
    # 2) ä½å…ƒçµ„é–€æª»
    b = 0
    out_chars: List[str] = []
    for ch in s:
        ch_b = len(ch.encode("utf-8"))
        if b + ch_b > threshold:
            break
        out_chars.append(ch)
        b += ch_b
    base = "".join(out_chars).rstrip()
    base = re.sub(r"\s*\d+$", "", base).strip()
    # 3) å¼·åˆ¶æˆªæ–·
    if not base or base.isdigit():
        base = s
    if base == s:
        # ç§»é™¤æœ€å¾Œä¸€å€‹èªè©
        t = re.sub(r"\s*\S+\s*$", "", s).rstrip()
        base = t if t else (s[:-1] if len(s) > 0 else s)
    return base


def uniquify_key(base_key: str, truncation_map: Dict[str, TruncationMapEntry]) -> str:
    """è¦æ ¼ 6-4ï¼šåƒ…é‡å° key è‡ªèº«åšå–®å±¤ '(n)' éå¢ï¼Œç›´åˆ°å”¯ä¸€ã€‚"""
    if base_key not in truncation_map:
        return base_key
    pat = re.compile(rf"^{re.escape(base_key)}\s*\((\d+)\)$")
    used = {base_key}
    for k in truncation_map.keys():
        m = pat.match(k)
        if m:
            used.add(k)
    n = 2
    while True:
        cand = f"{base_key} ({n})"
        if cand not in used and cand not in truncation_map:
            return cand
        n += 1


# ===== F1 / F2 / F3 =====
def files_are_fully_identical(path_a: Path, path_b: Path) -> bool:
    """F1ï¼šå®Œå…¨é‡è¤‡åˆ¤å®šï¼ˆå…ˆæ¯” cleanedï¼Œå†æ¯”æ•´æª”é€å­—ï¼Œå« YAMLï¼‰ã€‚"""
    # é¦–å¥ cleaned æ¯”å°
    ln_a = first_nonempty_line(skip_yaml(read_lines(path_a)))
    ln_b = first_nonempty_line(skip_yaml(read_lines(path_b)))
    if clean_markdown_line(ln_a) != clean_markdown_line(ln_b):
        return False
    # æ•´æª”é€å­—
    with open(get_safe_path(str(path_a)), "r", encoding="utf-8", errors="ignore") as fa, \
         open(get_safe_path(str(path_b)), "r", encoding="utf-8", errors="ignore") as fb:
        return fa.read() == fb.read()


def serialize_full_sentence(sentence: str, existing_full_values: Iterable[str]) -> str:
    """F2ï¼šç”Ÿæˆå”¯ä¸€åŒ–çš„ full_sentence 'S (n)'ã€‚ä¸å« I/Oã€‚"""
    s = sentence
    existing = set(existing_full_values)
    if s not in existing:
        return s
    n = 2
    while True:
        cand = f"{s} ({n})"
        if cand not in existing:
            return cand
        n += 1


def apply_serialized_suffix_to_file_headline(path: Path, new_sentence: str) -> None:
    """F2ï¼šæŠŠ '(n)' åŒæ­¥å¯«å›æª”æ¡ˆé¦–å¥ï¼ˆå”¯ä¸€å…è¨±çš„å…§å®¹è®Šæ›´ï¼‰ã€‚"""
    p = get_safe_path(str(path))
    lines = read_lines(path)
    # æ‰¾ YAML çµæŸ
    start_idx = 0
    if lines and lines[0].strip() == "---":
        for i in range(1, len(lines)):
            if lines[i].strip() == "---":
                start_idx = i + 1
                break
    # æ‰¾ç¬¬ä¸€å€‹éç©ºè¡Œ
    idx = None
    for i in range(start_idx, len(lines)):
        if lines[i].strip():
            idx = i
            break
    if idx is None:
        # ç©ºæª”ï¼šç›´æ¥æ’å…¥ä¸€è¡Œ
        lines.append(new_sentence + "\n")
    else:
        lines[idx] = new_sentence + "\n"
    with open(p, "w", encoding="utf-8") as f:
        f.writelines(lines)


def get_unused_uid(vault_path: str, start_index: int) -> Tuple[str, int]:
    """F3ï¼šæ–° UID æŒ‡æ´¾ã€‚éœ€åŒæ™‚æª¢æŸ¥ map èˆ‡æ•´å€‹ Vault æª”åï¼Œä¿è­‰å…¨åŸŸå”¯ä¸€ã€‚"""
    idx = max(1, int(start_index))
    while True:
        uid = f"uid_{idx:03d}"
        target = f"{uid}.md"
        found = False
        for root, _, files in os.walk(os.path.abspath(vault_path)):
            if target in files:
                found = True
                break
        if not found:
            return uid, idx + 1
        idx += 1


# ===== Rename / Temp Utilities =====
def ensure_global_unique_rename(src: Path, dest: Path) -> None:
    """å®‰å…¨æ­£åï¼ˆå¿…è¦æ™‚å…ˆæŠŠå ç”¨è€…ç§» temp æˆ–ç”¨ä¸­ç¹¼åå†æ›åï¼Œé¿å…è¦†å¯«ï¼‰ã€‚"""
    src_p = Path(get_safe_path(str(src)))
    dest_p = Path(get_safe_path(str(dest)))
    os.makedirs(os.path.dirname(str(dest_p)), exist_ok=True)
    if dest_p.exists():
        move_to_temp_name(dest_p)  # å…ˆæŠŠå ç”¨è€…è®“ä½
    os.rename(str(src_p), str(dest_p))


def move_to_temp_name(path: Path) -> Path:
    """å°‡æª”åæ›æˆ uid_fix_temp(n).mdï¼ˆn=1,2,3â€¦ å”¯ä¸€ï¼‰ã€‚"""
    parent = Path(str(path)).parent
    n = 1
    while True:
        cand = parent / f"uid_fix_temp({n}).md"
        cand_p = Path(get_safe_path(str(cand)))
        if not cand_p.exists():
            os.rename(get_safe_path(str(path)), str(cand_p))
            return cand_p
        n += 1


def uid_for_path(path: Path) -> Optional[str]:
    """è‹¥æª”åç‚º uid_XXX.md å›å‚³ uid_XXXï¼Œå¦å‰‡ Noneã€‚"""
    name = path.name
    m = re.fullmatch(r"(uid_\d{3,})\.md", name)
    return m.group(1) if m else None


def is_temp_file(path: Path) -> bool:
    """åˆ¤æ–·æ˜¯å¦ uid_fix_temp(n).mdã€‚"""
    return re.fullmatch(r"uid_fix_temp\(\d+\)\.md", path.name) is not None


# ===== Case Dispatchers (all orchestrators only) =====
def handle_uid_named_file(
    path: Path,
    cleaned: str,
    truncation_map: Dict[str, TruncationMapEntry],
    indices: Indices,
    stats: Stats,
    logger: Logger,
) -> None:
    """Case Aï¼šæª”åç‚º uid_XXX.mdã€‚
    â†’ ç›´æ¥é€²å…¥ã€å…±ç”¨é‚è¼¯ã€ï¼›å¦‚éœ€ï¼Œä¾è¦æ ¼å…ˆæ¬ç§»å ç”¨è€…ã€æ­£åã€æˆ–åºè™ŸåŒ–ï¼‹æ–° UIDã€‚
    """
    common_logic_with_cleaned(
        path=path,
        cleaned=cleaned,
        from_uid_named=True,
        truncation_map=truncation_map,
        indices=indices,
        stats=stats,
        logger=logger,
    )


def handle_general_named_file(
    path: Path,
    base_filename: str,
    cleaned: str,
    truncation_map: Dict[str, TruncationMapEntry],
    indices: Indices,
    stats: Stats,
    logger: Logger,
) -> None:
    """Case Bï¼šä¸€èˆ¬æª”åï¼ˆé uid é–‹é ­ï¼‰ã€‚
    1) èªæ„æ–·å¥ compare_filename_and_line
    2) è¢«æˆªæ–· is_truncated
    3) è‹¥çš†ç‚ºæ˜¯ â†’ é€²å…¥ã€å…±ç”¨é‚è¼¯ã€ï¼›éœ€è¦æ™‚å…ˆè½‰ç‚º uid_XXX.mdã€‚
    """
    ok, reason = compare_filename_and_line(base_filename, cleaned)
    if not ok:
        log_event(logger, action="skip-nonsegbreak", src=path, detail=reason)
        return

    filename_clean = remove_trailing_number(base_filename)
    truncated, reason_trunc = is_truncated(filename_clean, cleaned, LONG_FILENAME_UTF8_BYTES_THRESHOLD)
    log_event(logger, action="truncation-check", src=path, detail=reason_trunc)
    if not truncated:
        return

    # é€²å…¥å…±ç”¨é‚è¼¯ï¼ˆä¾†è·¯ç‚ºä¸€èˆ¬æª”åï¼‰
    common_logic_with_cleaned(
        path=path,
        cleaned=cleaned,
        from_uid_named=False,
        truncation_map=truncation_map,
        indices=indices,
        stats=stats,
        logger=logger,
    )


def handle_temp_file(
    path: Path,
    cleaned: str,
    truncation_map: Dict[str, TruncationMapEntry],
    indices: Indices,
    stats: Stats,
    logger: Logger,
) -> None:
    """Case Cï¼šuid_fix_temp(n).md â†’ ä¾è¦æ ¼è½‰æ­£ï¼ˆæˆ–åˆªé™¤å†—é¤˜ï¼‰ã€‚
    å®Œæˆè½‰æ­£å¾Œä¸é•·ç•™ tempï¼ˆæ­£åå¾Œè‡ªç„¶æ¶ˆå¤±ï¼‰ã€‚
    """
    expected_uid = indices.uid_for_full(cleaned)
    parent = path.parent

    if expected_uid:
        expected_path = parent / f"{expected_uid}.md"
        if expected_path.exists():
            # é¦–å¥ä¸€è‡´ï¼Ÿâ†’ F1
            ln_expected = first_nonempty_line(skip_yaml(read_lines(expected_path)))
            if clean_markdown_line(ln_expected) == cleaned:
                if files_are_fully_identical(path, expected_path):
                    # å†—é¤˜æš«å­˜ â†’ åˆªé™¤
                    os.remove(get_safe_path(str(path)))
                    stats.inc_deleted_dup()
                    log_event(logger, action="delete-duplicate-temp", src=path, dst=expected_path)
                    return
                else:
                    # åŒé¦–å¥ä¸åŒå…§å®¹ â†’ F2 + F3 æ–° UIDï¼Œæ”¹åã€æ–°å¢æ¢ç›®ï¼›key ä»¥ expected çš„ key ç‚º base éå¢
                    new_full = serialize_full_sentence(cleaned, indices.full_to_uid.keys())
                    if new_full != cleaned:
                        apply_serialized_suffix_to_file_headline(path, new_full)
                    # ä»¥ mapï¼ˆå…¨åŸŸï¼‰èˆ‡ç•¶å‰è³‡æ–™å¤¾ç¢ºä¿å”¯ä¸€ UID
                    used = set(indices.uid_to_expected_full.keys())
                    n = 1
                    while True:
                        cand = f"uid_{n:03d}"
                        if cand not in used and not (parent / f"{cand}.md").exists():
                            new_uid = cand
                            break
                        n += 1
                    # æ”¹å
                    dest = parent / f"{new_uid}.md"
                    ensure_global_unique_rename(path, dest)
                    # key åŸºæ–¼ expected çš„æ—¢æœ‰ key éå¢
                    base_key = indices.key_for_uid(expected_uid) or synthesize_truncation_key_from_cleaned(cleaned)
                    key = uniquify_key(base_key, truncation_map)
                    add_map_entry(truncation_map, key, new_uid, new_full, indices)
                    stats.inc_serialized(); stats.inc_new_uid(); stats.inc_renamed(); stats.inc_added()
                    log_event(logger, action="temp-serialize-newuid", src=path, dst=dest, detail=f"key={key}")
                    return
            else:
                # é¦–å¥ä¸åŒï¼šæ­¤ temp èˆ‡ expected ç„¡é—œ â†’ è¦–ç‚ºæ–°å…§å®¹ï¼ˆF2 è¦–éœ€æ±‚ï¼‰+ æ–° UID
                pass  # è½åˆ°ä¸‹é¢ã€Œexpected_uid ä¸å­˜åœ¨æˆ–ä¸é©ç”¨ã€çš„è·¯å¾‘

        # expected_uid.md ä¸å­˜åœ¨ï¼ˆmap éºå¤±å¯¦é«”ï¼‰â†’ ä¸å¾—ç›´æ¥å ç”¨è©² uidï¼›è¦–ç‚ºæ–°å¢å…§å®¹
        stats.inc_orphan()
        # F2ï¼ˆå¿…è¦ï¼‰+ F3
        new_full = serialize_full_sentence(cleaned, indices.full_to_uid.keys())
        if new_full != cleaned:
            apply_serialized_suffix_to_file_headline(path, new_full)

        used = set(indices.uid_to_expected_full.keys())
        n = 1
        while True:
            cand = f"uid_{n:03d}"
            if cand not in used and not (parent / f"{cand}.md").exists():
                new_uid = cand
                break
            n += 1
        dest = parent / f"{new_uid}.md"
        ensure_global_unique_rename(path, dest)
        base_key = indices.key_for_uid(expected_uid) or synthesize_truncation_key_from_cleaned(cleaned)
        key = uniquify_key(base_key, truncation_map)
        add_map_entry(truncation_map, key, new_uid, new_full, indices)
        stats.inc_new_uid(); stats.inc_renamed(); stats.inc_added()
        log_event(logger, action="temp-newuid-orphan-map", src=path, dst=dest, detail=f"key={key}")
        return

    # cleaned ä¸åœ¨ map â†’ æ–°å…§å®¹ï¼šF2ï¼ˆå¦‚éœ€ï¼‰ï¼‹ F3
    new_full = cleaned  # è‹¥ map ç„¡æ­¤å¥ï¼Œé€šå¸¸ä¸éœ€åºè™ŸåŒ–
    if new_full in indices.full_to_uid:
        new_full = serialize_full_sentence(new_full, indices.full_to_uid.keys())
        if new_full != cleaned:
            apply_serialized_suffix_to_file_headline(path, new_full)

    used = set(indices.uid_to_expected_full.keys())
    n = 1
    while True:
        cand = f"uid_{n:03d}"
        if cand not in used and not (parent / f"{cand}.md").exists():
            new_uid = cand
            break
        n += 1
    dest = parent / f"{new_uid}.md"
    ensure_global_unique_rename(path, dest)

    base_key = synthesize_truncation_key_from_cleaned(new_full)
    key = uniquify_key(base_key, truncation_map)
    add_map_entry(truncation_map, key, new_uid, new_full, indices)
    stats.inc_new_uid(); stats.inc_renamed(); stats.inc_added()
    log_event(logger, action="temp-newuid-fresh", src=path, dst=dest, detail=f"key={key}")


# ===== Shared Logic (cleaned in map? ) =====
def common_logic_with_cleaned(
    path: Path,
    cleaned: str,
    from_uid_named: bool,
    truncation_map: Dict[str, TruncationMapEntry],
    indices: Indices,
    stats: Stats,
    logger: Logger,
) -> None:
    """å…±ç”¨é‚è¼¯ï¼ˆè¦æ ¼ï¼šä»¥ cleaned æ˜¯å¦åœ¨ map åˆ†æµï¼›a) ä¾†è‡ª uid æª”ï¼›b) ä¾†è‡ªé uid æª”ï¼‰
    (1) cleaned å·²åœ¨ map â†’ expected_uid åˆ†æ”¯ï¼ˆF1 / è½‰ temp / æ­£åï¼‰
    (2) cleaned ä¸åœ¨ map â†’ æ–°å…§å®¹ï¼š
        - ä¾†è·¯ b) é uid æª” â†’ F3 æ–° UID â†’ æ”¹æª”å â†’ key=åŸå§‹è¢«æˆªæ–·æª”å(é è™•ç†å¾Œ) â†’ æ–°å¢æ¢ç›®
        - ä¾†è·¯ a) uid æª” â†’ ä¾ã€ä»¥ uid æ”¶éŒ„æ™‚ key è¦å‰‡ã€åˆæˆ key â†’ å¿…æ”¶éŒ„ or è¡çªæ”¹ temp å¾Œ Case C
    """
    parent = path.parent
    expected_uid = indices.uid_for_full(cleaned)

    # (1) cleaned å·²åœ¨ map
    if expected_uid:
        expected_path = parent / f"{expected_uid}.md"

        if from_uid_named:
            current_uid = uid_for_path(path)
            if current_uid == expected_uid:
                # ä¸€è‡´ â†’ ç„¡å‹•ä½œ
                log_event(logger, action="noop-consistent", src=path)
                return

        if expected_path.exists():
            # æ¯”å°é¦–å¥
            ln_expected = first_nonempty_line(skip_yaml(read_lines(expected_path)))
            if clean_markdown_line(ln_expected) == cleaned:
                # F1ï¼šå®Œå…¨é‡è¤‡ï¼Ÿ
                if files_are_fully_identical(path, expected_path):
                    # å†—é¤˜ â†’ åˆªé™¤ç•¶å‰æª”æ¡ˆ
                    os.remove(get_safe_path(str(path)))
                    stats.inc_deleted_dup()
                    log_event(logger, action="delete-duplicate", src=path, dst=expected_path)
                    return
                else:
                    # å…©è€…å…±å­˜ â†’ å°ã€Œç•¶å‰æª”æ¡ˆã€åºè™ŸåŒ– + æ–° UID + æ–°æ¢ç›®ï¼›key ä»¥ expected çš„ key ç‚º base éå¢
                    new_full = serialize_full_sentence(cleaned, indices.full_to_uid.keys())
                    if new_full != cleaned:
                        apply_serialized_suffix_to_file_headline(path, new_full)
                    # æŒ‡æ´¾æ–° UIDï¼ˆä»¥ map + ç•¶å‰è³‡æ–™å¤¾æª¢æŸ¥ï¼‰
                    used = set(indices.uid_to_expected_full.keys())
                    n = 1
                    while True:
                        cand = f"uid_{n:03d}"
                        if cand not in used and not (parent / f"{cand}.md").exists():
                            new_uid = cand
                            break
                        n += 1
                    dest = parent / f"{new_uid}.md"
                    ensure_global_unique_rename(path, dest)

                    base_key = indices.key_for_uid(expected_uid) or synthesize_truncation_key_from_cleaned(cleaned)
                    key = uniquify_key(base_key, truncation_map)
                    add_map_entry(truncation_map, key, new_uid, new_full, indices)
                    stats.inc_serialized(); stats.inc_new_uid(); stats.inc_renamed(); stats.inc_added()
                    log_event(logger, action="serialize-newuid", src=path, dst=dest, detail=f"key={key}")
                    return
            else:
                # é¦–å¥ä¸åŒ â†’ å ç”¨è€…éœ€è¦æ›´æ­£æª”åï¼šå…ˆæŠŠå ç”¨è€…ç§» tempï¼Œå†æŠŠç•¶å‰æª”æ¡ˆæ­£åç‚º expected_uid.md
                moved = move_to_temp_name(expected_path)
                log_event(logger, action="preempt-occupier-to-temp", src=expected_path, dst=moved)
                dest = parent / f"{expected_uid}.md"
                ensure_global_unique_rename(path, dest)
                stats.inc_renamed()
                log_event(logger, action="rename-to-expected-uid", src=path, dst=dest)
                return
        else:
            # ç„¡äººå ç”¨ â†’ ç›´æ¥æ­£åç‚º expected_uid.mdï¼ˆä¸æ”¹å…§å®¹ï¼‰
            dest = parent / f"{expected_uid}.md"
            ensure_global_unique_rename(path, dest)
            stats.inc_renamed()
            log_event(logger, action="rename-to-expected-uid", src=path, dst=dest)
            return

    # (2) cleaned ä¸åœ¨ map â†’ æ–°å…§å®¹
    if from_uid_named:
        # ä¾†è·¯ a) uid æª”ï¼šå¿…æ”¶éŒ„æˆ–è¡çªè½‰ tempï¼ˆç”± Case C å¾ŒçºŒè™•ç†ï¼‰
        current_uid = uid_for_path(path)
        if current_uid and not indices.has_uid(current_uid):
            # å¿…æ”¶éŒ„ï¼šä»¥ cleaned åæ¨æ¨™æº– key â†’ å”¯ä¸€åŒ– â†’ æ–°å¢æ¢ç›®
            base_key = synthesize_truncation_key_from_cleaned(cleaned)
            key = uniquify_key(base_key, truncation_map)
            add_map_entry(truncation_map, key, current_uid, cleaned, indices)
            stats.inc_added()
            log_event(logger, action="register-uid-file", src=path, detail=f"key={key}")
        else:
            # UID è¡çªï¼ˆmap å·²ä½¿ç”¨æ­¤ UIDï¼‰â†’ æ”¹åç‚º tempï¼Œç•™å¾… Case C
            moved = move_to_temp_name(path)
            log_event(logger, action="uid-conflict-move-temp", src=path, dst=moved)
        return

    # ä¾†è·¯ b) é uid æª”ï¼šF3 æ–° UID â†’ æ”¹æª”å â†’ key=åŸå§‹è¢«æˆªæ–·æª”å(é è™•ç†å¾Œ) â†’ æ–°å¢æ¢ç›®
    used = set(indices.uid_to_expected_full.keys())
    n = 1
    while True:
        cand = f"uid_{n:03d}"
        if cand not in used and not (parent / f"{cand}.md").exists():
            new_uid = cand
            break
        n += 1

    dest = parent / f"{new_uid}.md"
    ensure_global_unique_rename(path, dest)

    base_key = remove_trailing_number(path.stem)  # æª”åé è™•ç†å¾Œä½œç‚º key base
    key = uniquify_key(base_key, truncation_map)
    add_map_entry(truncation_map, key, new_uid, cleaned, indices)
    stats.inc_new_uid(); stats.inc_renamed(); stats.inc_added()
    log_event(logger, action="general-newuid", src=path, dst=dest, detail=f"key={key}")



# ===== Map Mutations =====
def add_map_entry(
    truncation_map: Dict[str, TruncationMapEntry],
    key: str,
    uid: str,
    full_sentence: str,
    indices: Indices,
) -> None:
    """æ–°å¢ map æ¢ç›®ï¼ˆä¸‰é‡å”¯ä¸€ï¼škey/uid/full_sentenceï¼‰ã€‚åŒæ­¥åˆ·æ–° indicesã€‚"""
    # ä¸‰é‡å”¯ä¸€æ€§æª¢æŸ¥ï¼ˆä¸è¦†å¯«ã€ä¸æŒªç”¨ï¼‰
    if key in truncation_map:
        raise ValueError(f"map key already exists: {key}")
    if uid in indices.uid_to_expected_full:
        raise ValueError(f"uid already exists in map: {uid}")
    if full_sentence in indices.full_to_uid:
        raise ValueError(f"full_sentence already exists in map: {full_sentence}")

    entry = TruncationMapEntry(uid=uid, full_sentence=full_sentence)
    truncation_map[key] = entry
    # ä¿æŒä¸‰ç´¢å¼•åŒæ­¥
    indices.register(key, entry)


# ===== Logging =====
def log_params(logger: Logger) -> None:
    """åœ¨ log é–‹é ­åˆ—å°ä¸»è¦åƒæ•¸ï¼ˆå¦‚ bytes é–€æª»ç­‰ï¼‰ã€‚"""
    logger.log("=== build_uid_map_for_truncated_titles: run params ===")
    logger.log(f"- LONG_FILENAME_UTF8_BYTES_THRESHOLD = {LONG_FILENAME_UTF8_BYTES_THRESHOLD}")
    logger.log("=====================================================")


def log_event(
    logger: Logger, *, action: str, src: Optional[Path] = None, dst: Optional[Path] = None, detail: str = ""
) -> None:
    """çµ±ä¸€äº‹ä»¶è¨˜éŒ„ï¼šrename/delete/serialize/audit ç­‰ï¼Œéœ€åŒ…å«ä¾†æºèˆ‡ç›®çš„ç›¸å°è·¯å¾‘ã€‚"""
    parts = [f"[{action}]"]
    if src is not None:
        parts.append(f"src={src}")
    if dst is not None:
        parts.append(f"dst={dst}")
    if detail:
        parts.append(f"info={detail}")
    logger.log(" ".join(parts))

def log_stats_summary(
    logger: Logger,
    stats: Stats,
    truncation_map: Dict[str, TruncationMapEntry],
    *,
    map_count_before: Optional[int] = None,
) -> None:
    """åœ¨ log çµå°¾è¼¸å‡ºçµ±è¨ˆç¸½è¡¨ï¼ˆå« map è¦æ¨¡è®ŠåŒ–ï¼‰ã€‚"""
    after = len(truncation_map)
    delta = None if map_count_before is None else (after - map_count_before)

    logger.log("")  # ç©ºè¡Œåˆ†éš”
    logger.log("ğŸ“Š Summary (this run)")
    if map_count_before is not None:
        logger.log(f"  - map_entries_before: {map_count_before}")
        logger.log(f"  - map_entries_after : {after}")
        logger.log(f"  - map_entries_delta : {delta:+d}")
    else:
        logger.log(f"  - map_entries_after : {after}")

    logger.log("  - new_uid_assigned          : {}".format(stats.new_uid_assigned))
    logger.log("  - supplemental_entries_added: {}".format(stats.supplemental_entries_added))
    logger.log("  - renames_to_uid            : {}".format(stats.renames_to_uid))
    logger.log("  - duplicates_deleted        : {}".format(stats.duplicates_deleted))
    logger.log("  - conflicts_serialized      : {}".format(stats.conflicts_serialized))
    logger.log("  - temps_repaired            : {}".format(stats.temps_repaired))
    logger.log("  - orphan_targets_seen       : {}".format(stats.orphan_targets_seen))


# ===== Orchestrator =====
def build_uid_map_for_truncated_titles(
    vault_path: str, map_path: str, log_path: str, verbose: bool = False
) -> Dict[str, TruncationMapEntry]:
    """
    ä¸»æµç¨‹ï¼ˆåƒ…å‘¼å«ï¼Œç„¡å¯¦ä½œé‚è¼¯ï¼‰ï¼š
    1) å»º loggerã€åˆ—å°åƒæ•¸
    2) è®€ map â†’ å»ºç´¢å¼•
    3) ç¬¬ä¸€è¼ªéæ­·ï¼ˆCase A/Bï¼‰ï¼š
       - è·³é temp
       - è®€æª”ã€skip_yamlã€å–ç¬¬ä¸€è¡Œã€clean
       - ä¾æª”åé¡å‹åˆ†æ´¾ï¼šuid / ä¸€èˆ¬
    4) é‡å»ºç´¢å¼•ï¼ˆç¬¬ä¸€è¼ªå·²æ”¹ map/æª”åï¼‰
    5) ç¬¬äºŒè¼ªéæ­·ï¼ˆCase Cï¼‰ï¼šåªè™•ç† temp
    6) å„²å­˜ mapã€å¯« logã€è¼¸å‡ºçµ±è¨ˆ
    """
    logger = Logger(log_path=log_path, verbose=verbose, title=None)
    log_params(logger)

    truncation_map = load_truncation_map(get_safe_path(map_path))
    map_count_before = len(truncation_map) 
    indices = build_indices_from_map(truncation_map)
    stats = Stats()

    # ---------- Pass 1: Case A / B ----------
    for path in iter_vault_md_files(vault_path):
        if is_temp_file(path):
            continue  # ç¬¬ä¸€è¼ªè·³é Case C

        lines = read_lines(path)
        content_lines = skip_yaml(lines)
        line = first_nonempty_line(content_lines)
        cleaned = clean_markdown_line(line)
        base_filename = path.stem

        if (uid := uid_for_path(path)) is not None:
            handle_uid_named_file(path, cleaned, truncation_map, indices, stats, logger)     # Case A
        else:
            handle_general_named_file(path, base_filename, cleaned, truncation_map, indices, stats, logger)  # Case B

    # ï¼ˆå¯é¸ï¼‰Checkpointï¼šç¬¬ä¸€è¼ªå¾Œå…ˆå­˜ä¸€æ¬¡ï¼Œä¾¿æ–¼å¾©åŸï¼å¯©è¨ˆ
    # save_truncation_map(get_safe_path(map_path), truncation_map)
    # logger.save()

    # é‡è¦ï¼šç¬¬ä¸€è¼ªå¯èƒ½æ”¹äº† map/æª”åï¼Œç¬¬äºŒè¼ªå‰é‡å»ºç´¢å¼•
    indices = build_indices_from_map(truncation_map)

    # ---------- Pass 2: Case C ----------
    for path in iter_vault_md_files(vault_path):
        if not is_temp_file(path):
            continue  # åªè™•ç† temp

        lines = read_lines(path)
        content_lines = skip_yaml(lines)
        line = first_nonempty_line(content_lines)
        cleaned = clean_markdown_line(line)

        handle_temp_file(path, cleaned, truncation_map, indices, stats, logger)  # Case C

    log_stats_summary(logger, stats, truncation_map, map_count_before=map_count_before)
    save_truncation_map(get_safe_path(map_path), truncation_map)
    logger.save()
    return truncation_map



# ===== CLI Entrypoint =====
if __name__ == "__main__":
    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    VAULT_DIR = os.path.join(BASE_DIR, "TestData")
    MAP_PATH = os.path.join(BASE_DIR, "log", "truncation_map.json")
    LOG_PATH = os.path.join(BASE_DIR, "log", "truncation_detect.log")

    os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
    # åªç¤ºæ„å‘¼å«ï¼Œä¸åšä»»ä½•å¯¦ä½œ
    build_uid_map_for_truncated_titles(VAULT_DIR, MAP_PATH, LOG_PATH, verbose=True)
